{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2e29762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.2571879206299053\n",
      "Epoch 1000, Loss: 0.010484309641948516\n",
      "Epoch 2000, Loss: 0.003379559793611682\n",
      "Epoch 3000, Loss: 0.0015234291253224275\n",
      "Epoch 4000, Loss: 0.0007829631575888029\n",
      "Epoch 5000, Loss: 0.000429781392090137\n",
      "Epoch 6000, Loss: 0.00024497472409881104\n",
      "Epoch 7000, Loss: 0.00014291453999368096\n",
      "Epoch 8000, Loss: 8.464304234519455e-05\n",
      "Epoch 9000, Loss: 5.065531220176381e-05\n",
      "Final Output after Training:\n",
      "[[0.0102778 ]\n",
      " [0.9976088 ]\n",
      " [0.00203539]\n",
      " ...\n",
      " [0.00203539]\n",
      " [0.00203539]\n",
      " [0.00203539]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Xor_Dataset.csv\")\n",
    "\n",
    "# Extract inputs (x, y) and outputs (z)\n",
    "X = data[['X', 'Y']].values  # Inputs: x, y\n",
    "y = data['Z'].values.reshape(-1, 1)  # Output: z (reshaped to column vector)\n",
    "\n",
    "# Activation functions: ReLU for hidden, Sigmoid for output\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Adam optimizer parameters\n",
    "beta1 = 0.9  # Exponential decay rate for first moment estimate\n",
    "beta2 = 0.999  # Exponential decay rate for second moment estimate\n",
    "epsilon = 1e-8  # Small constant to prevent division by zero\n",
    "learning_rate = 0.001  # Learning rate\n",
    "input_layer_neurons = 2  # XOR input has 2 features (x, y)\n",
    "hidden_layer_neurons = 6  # Increased hidden neurons (to improve capacity)\n",
    "output_layer_neurons = 1  # XOR output is a single binary value\n",
    "\n",
    "# Xavier Initialization for weights\n",
    "def initialize_weights(input_size, output_size):\n",
    "    limit = np.sqrt(6 / (input_size + output_size))\n",
    "    return np.random.uniform(-limit, limit, (input_size, output_size))\n",
    "\n",
    "# Initialize weights and biases using Xavier initialization\n",
    "weights_input_hidden = initialize_weights(input_layer_neurons, hidden_layer_neurons)\n",
    "bias_hidden = np.zeros((1, hidden_layer_neurons))\n",
    "\n",
    "weights_hidden_output = initialize_weights(hidden_layer_neurons, output_layer_neurons)\n",
    "bias_output = np.zeros((1, output_layer_neurons))\n",
    "\n",
    "# Adam optimizer momentums\n",
    "m_weights_input_hidden = np.zeros_like(weights_input_hidden)\n",
    "v_weights_input_hidden = np.zeros_like(weights_input_hidden)\n",
    "m_bias_hidden = np.zeros_like(bias_hidden)\n",
    "v_bias_hidden = np.zeros_like(bias_hidden)\n",
    "\n",
    "m_weights_hidden_output = np.zeros_like(weights_hidden_output)\n",
    "v_weights_hidden_output = np.zeros_like(weights_hidden_output)\n",
    "m_bias_output = np.zeros_like(bias_output)\n",
    "v_bias_output = np.zeros_like(bias_output)\n",
    "\n",
    "# Training the network using Feedforward and Backpropagation with Adam Optimizer\n",
    "epochs = 10000\n",
    "for epoch in range(epochs):\n",
    "    # Feedforward Propagation:\n",
    "\n",
    "    # Hidden layer input\n",
    "    hidden_layer_input = np.dot(X, weights_input_hidden) + bias_hidden\n",
    "    hidden_layer_output = relu(hidden_layer_input)\n",
    "\n",
    "    # Output layer input\n",
    "    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output\n",
    "    output_layer_output = sigmoid(output_layer_input)\n",
    "\n",
    "    # Calculate the error (Loss Function: Mean Squared Error)\n",
    "    error = y - output_layer_output\n",
    "    loss = np.mean(np.square(error))\n",
    "\n",
    "    # Backpropagation (with Adam optimization)\n",
    "\n",
    "    # Output layer error and delta\n",
    "    output_layer_error = error\n",
    "    output_layer_delta = output_layer_error * sigmoid_derivative(output_layer_output)\n",
    "\n",
    "    # Hidden layer error and delta\n",
    "    hidden_layer_error = output_layer_delta.dot(weights_hidden_output.T)\n",
    "    hidden_layer_delta = hidden_layer_error * relu_derivative(hidden_layer_output)\n",
    "\n",
    "    # Adam Update for weights and biases\n",
    "    m_weights_input_hidden = beta1 * m_weights_input_hidden + (1 - beta1) * np.dot(X.T, hidden_layer_delta)\n",
    "    v_weights_input_hidden = beta2 * v_weights_input_hidden + (1 - beta2) * np.dot(X.T, hidden_layer_delta)**2\n",
    "    m_weights_hidden_output = beta1 * m_weights_hidden_output + (1 - beta1) * np.dot(hidden_layer_output.T, output_layer_delta)\n",
    "    v_weights_hidden_output = beta2 * v_weights_hidden_output + (1 - beta2) * np.dot(hidden_layer_output.T, output_layer_delta)**2\n",
    "\n",
    "    m_bias_hidden = beta1 * m_bias_hidden + (1 - beta1) * np.sum(hidden_layer_delta, axis=0, keepdims=True)\n",
    "    v_bias_hidden = beta2 * v_bias_hidden + (1 - beta2) * np.sum(hidden_layer_delta, axis=0, keepdims=True)**2\n",
    "    m_bias_output = beta1 * m_bias_output + (1 - beta1) * np.sum(output_layer_delta, axis=0, keepdims=True)\n",
    "    v_bias_output = beta2 * v_bias_output + (1 - beta2) * np.sum(output_layer_delta, axis=0, keepdims=True)**2\n",
    "\n",
    "    # Bias and weights update using Adam\n",
    "    weights_input_hidden += learning_rate * m_weights_input_hidden / (np.sqrt(v_weights_input_hidden) + epsilon)\n",
    "    weights_hidden_output += learning_rate * m_weights_hidden_output / (np.sqrt(v_weights_hidden_output) + epsilon)\n",
    "\n",
    "    bias_hidden += learning_rate * m_bias_hidden / (np.sqrt(v_bias_hidden) + epsilon)\n",
    "    bias_output += learning_rate * m_bias_output / (np.sqrt(v_bias_output) + epsilon)\n",
    "\n",
    "    # Print the loss every 1000 epochs to observe the convergence\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Final output after training\n",
    "print(\"Final Output after Training:\")\n",
    "print(output_layer_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09f17083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a6a488f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      X  Y  Z\n",
      "0     0  0  0\n",
      "1     0  1  1\n",
      "2     1  1  0\n",
      "3     1  1  0\n",
      "4     0  0  0\n",
      "...  .. .. ..\n",
      "9995  0  0  0\n",
      "9996  0  1  1\n",
      "9997  1  1  0\n",
      "9998  1  1  0\n",
      "9999  1  1  0\n",
      "\n",
      "[10000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"Xor_Dataset.csv\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f80df56",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=data[[\"X\",\"Y\"]].to_numpy()\n",
    "expected_output=data[[\"Z\"]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2be44b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a429bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_sigmoid(x):\n",
    "    return x*(1-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f39dfc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7cd101d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f26c8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "input_layer_neuron=2\n",
    "hidden_layer1_neuron=6\n",
    "hidden_layer2_neuron=4\n",
    "outer_layer_neuron=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c90d94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Cross-Entropy Loss\n",
    "def binary_cross_entropy_loss(y_true, y_pred):\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0921071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "[[0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#for hidden_layer_1\n",
    "weight_input_hidden1 = np.random.randn(input_layer_neuron, hidden_layer1_neuron) * np.sqrt(2. / input_layer_neuron)\n",
    "bias_hidden1 = np.zeros((1, hidden_layer1_neuron))\n",
    "\n",
    "#for hidden_layer_2\n",
    "weight_hidden1_hidden2 = np.random.randn(hidden_layer1_neuron, hidden_layer2_neuron) * np.sqrt(2. / hidden_layer1_neuron)\n",
    "bias_hidden2 = np.zeros((1, hidden_layer2_neuron))\n",
    "\n",
    "#for output_layer\n",
    "weight_hidden2_output = np.random.randn(hidden_layer2_neuron, output_layer_neuron) * np.sqrt(1. / hidden_layer2_neuron)\n",
    "bias_output = np.zeros((1, output_layer_neuron))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510e0251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(inputs):\n",
    "    #hidden_layer_1\n",
    "    hidden1_input=np.dot(inputs,weight_input_hidden1)+bias_hidden1\n",
    "    hidden1_output=relu(hidden1_input)\n",
    "    #hidden_layer_2\n",
    "    hidden2_input=np.dot(weight_input_hidden1,weight_hidden1_hidden2)_bias_hidden2\n",
    "    hidden2_output=relu(hidden2_input)\n",
    "    #output_layer\n",
    "    output_input=np.dot(weight_input_hidden2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7615fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7365458679333212\n",
      "Epoch 1000, Loss: 0.48382479561878144\n",
      "Epoch 2000, Loss: 0.4830257042158113\n",
      "Epoch 3000, Loss: 0.48302586513784646\n",
      "Epoch 4000, Loss: 0.4826592511730427\n",
      "Epoch 5000, Loss: 0.4824799320898663\n",
      "Epoch 6000, Loss: 0.482690820579996\n",
      "Epoch 7000, Loss: 0.4824523018937604\n",
      "Epoch 8000, Loss: 0.4823368411270143\n",
      "Epoch 9000, Loss: 0.48226591160173904\n",
      "Final Output after Training:\n",
      "[[5.74179428e-04]\n",
      " [6.66578159e-01]\n",
      " [6.66578159e-01]\n",
      " ...\n",
      " [6.66578159e-01]\n",
      " [6.66578159e-01]\n",
      " [6.66578159e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Training the network using Feedforward and Backpropagation with Adam Optimizer\n",
    "epochs = 10000\n",
    "for epoch in range(epochs):\n",
    "    # Feedforward Propagation:\n",
    "\n",
    "    # Hidden layer input\n",
    "    hidden_layer_input = np.dot(X, weights_input_hidden) + bias_hidden\n",
    "    hidden_layer_output = relu(hidden_layer_input)\n",
    "\n",
    "    # Output layer input\n",
    "    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output\n",
    "    output_layer_output = sigmoid(output_layer_input)\n",
    "\n",
    "    # Calculate the error (Loss Function: Binary Cross-Entropy)\n",
    "    loss = binary_cross_entropy_loss(y, output_layer_output)\n",
    "\n",
    "    # Backpropagation (with Adam optimization)\n",
    "    output_layer_error = y - output_layer_output\n",
    "    output_layer_delta = output_layer_error * sigmoid_derivative(output_layer_output)\n",
    "\n",
    "    hidden_layer_error = output_layer_delta.dot(weights_hidden_output.T)\n",
    "    hidden_layer_delta = hidden_layer_error * relu_derivative(hidden_layer_output)\n",
    "\n",
    "    # Adam optimizer update (same as before)\n",
    "    m_weights_input_hidden = beta1 * m_weights_input_hidden + (1 - beta1) * np.dot(X.T, hidden_layer_delta)\n",
    "    v_weights_input_hidden = beta2 * v_weights_input_hidden + (1 - beta2) * np.dot(X.T, hidden_layer_delta)**2\n",
    "    m_weights_hidden_output = beta1 * m_weights_hidden_output + (1 - beta1) * np.dot(hidden_layer_output.T, output_layer_delta)\n",
    "    v_weights_hidden_output = beta2 * v_weights_hidden_output + (1 - beta2) * np.dot(hidden_layer_output.T, output_layer_delta)**2\n",
    "\n",
    "    m_bias_hidden = beta1 * m_bias_hidden + (1 - beta1) * np.sum(hidden_layer_delta, axis=0, keepdims=True)\n",
    "    v_bias_hidden = beta2 * v_bias_hidden + (1 - beta2) * np.sum(hidden_layer_delta, axis=0, keepdims=True)**2\n",
    "    m_bias_output = beta1 * m_bias_output + (1 - beta1) * np.sum(output_layer_delta, axis=0, keepdims=True)\n",
    "    v_bias_output = beta2 * v_bias_output + (1 - beta2) * np.sum(output_layer_delta, axis=0, keepdims=True)**2\n",
    "\n",
    "    # Bias and weights update using Adam\n",
    "    weights_input_hidden += learning_rate * m_weights_input_hidden / (np.sqrt(v_weights_input_hidden) + epsilon)\n",
    "    weights_hidden_output += learning_rate * m_weights_hidden_output / (np.sqrt(v_weights_hidden_output) + epsilon)\n",
    "\n",
    "    bias_hidden += learning_rate * m_bias_hidden / (np.sqrt(v_bias_hidden) + epsilon)\n",
    "    bias_output += learning_rate * m_bias_output / (np.sqrt(v_bias_output) + epsilon)\n",
    "\n",
    "    # Print the loss every 1000 epochs to observe the convergence\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Final output after training\n",
    "print(\"Final Output after Training:\")\n",
    "print(output_layer_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044e52ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from train import XORModel\n",
    "\n",
    "def make_inference(inputs):\n",
    "    \"\"\"\n",
    "    Make predictions using the trained model.\n",
    "    \n",
    "    Args:\n",
    "    inputs (numpy array): Input data to make predictions.\n",
    "    \n",
    "    Returns:\n",
    "    numpy array: Predicted output values.\n",
    "    \"\"\"\n",
    "    model = XORModel()\n",
    "    # Load the trained model (we'll skip this part for now)\n",
    "    # model.load_model('model.save') \n",
    "    \n",
    "    predicted_output, _, _ = model.forward_propagation(inputs)\n",
    "    return predicted_output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
